{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AWS Timestream to read/write tick data\n",
    "\n",
    "October 2020 - Saeed Amen - https://www.cuemacro.com - saeed@cuemacro.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this Jupyter notebook, we expand on the blog article at https://www.cuemacro.com/2020/10/27/using-aws-timestream-for-tick-data/ on Timestream, with more technical details and code.\n",
    "\n",
    "We'll give simple example of how to use AWS Timestream. Timestream AWS's new serverless database designed for storing time series, in particular from IoT devices. Although, given that finance is awash with time series, I though it would be a good idea to see how it can handle high frequency financial market tick data in my example.\n",
    "\n",
    "Timestream is also fully managed so you don't need to install the database and it should scale without you having to worry about renting more servers etc. which you'd have to do if you managed it yourself. \n",
    "It has lots of built in functions for querying the data and has the Timestream Query Language (see https://docs.aws.amazon.com/timestream/latest/developerguide/reference.html), which looks pretty similar to SQL.\n",
    "\n",
    "Obviously, there are many other time series databases out there. There are already many others out including, kdb+/q which is heavily used in sell side firms/trading shops using high frequency tick data. There are also some open source time series databases like InfluxDB or Arctic (used with MongoDB). Many of these databases are also available in the cloud in a fully managed way too.\n",
    "\n",
    "As to which is cheaper, I haven't attempted to do any sort of cost analysis, although [AWS](https://aws.amazon.com/blogs/aws/store-and-access-time-series-data-at-any-scale-with-amazon-timestream-now-generally-available/) claims that: \n",
    "\n",
    "    Timestream is a fast, scalable, and serverless time series database service that makes it easy to collect, store, and process trillions of time series events per day up to 1,000 times faster and at as little as to 1/10th the cost of a relational database.\n",
    "\n",
    "\n",
    "My tcapy Python library, for transaction cost analysis of FX spot, actually has wrappers for reading/writing tick data using kdb+/q, InfluxDB and Arctic (download it for free from https://github.com/cuemacro/tcapy).\n",
    "\n",
    "### The idea of Timestream\n",
    "\n",
    "Timestream is a serverless database. It is also fully managed and you don't need to install the database. By having a serverless archtecture, it should hopefully scale with the data you store. You'll only pay for the space you use.\n",
    "\n",
    "It has lots of built in functions for querying the data and has the Timestream Query Language (see https://docs.aws.amazon.com/timestream/latest/developerguide/reference.html), which looks pretty similar to SQL.\n",
    "\n",
    "Timestream stores newer data in memory, whilst older data is stored on (cheaper) magnetic tape. Other time series databases do something similar, storing new data in RAM, whilst old data can be stored on disk.\n",
    "\n",
    "You can stipulate how long you want Timestream to store the data in either. To the user querying the data they don't need to worry about separating their queries for the memory/magnetic tape parts of the data. This is done automatically.\n",
    "\n",
    "### Restrictions on the time of writing data\n",
    "\n",
    "One important point is that you can't simply record data with any timestamp. [AWS notes](https://aws.amazon.com/blogs/aws/store-and-access-time-series-data-at-any-scale-with-amazon-timestream-now-generally-available/) that\n",
    "\n",
    "    When writing data in Timestream, you cannot insert data that is older than the retention period of the memory store. For example, in my case I will not be able to insert records older than 1 hour. Similarly, you cannot insert data with a future timestamp.\n",
    "\n",
    "### Record, Measure and Dimensions\n",
    "\n",
    "Each data point is a `Record` has a `Time` associated with it, and a `Measure`, which is like a database field/column and we specific a type for it. So if we think of tick data, the `Measure` could be the mid-price and the `Time` could be the time at which we write the data to Timestream. We can't specify multiple `Measure` fields. If we are storing tick data, however, we often need to store multiple values for the same time stamp.\n",
    "\n",
    "Each data point can also have `Dimensions` associated with it, which can have multiple name/value combinations. These will typically be things which don't change much, such as the ticker. \n",
    "\n",
    "### Storing multiple fields using Dimensions\n",
    "\n",
    "Whilst we can't specify multiple `Measure` fields in the same data point, we can point different `Measure` fields to the same `Dimensions`. For example, later, I'll store the `venue_time` in the `Dimensions` field. So we could have a bid value, ask value, mid value etc. all pointing to the same `Dimensions` name/value combinations. When querying the data, we could stich back these into the format we want. Having two `Time` fields might seem odd, however, in practice, this often happens when collecting tick data, you have the `venue_time` and the time when you snapped the data locally, and there's likely to be a gap between the two.\n",
    "\n",
    "At this stage, I'm not sure what the performance implications of using the `Dimensions` to store the `venue_time`, but it does seem like a relatively simple way of storing more complicated structures, where you have multiple fields in the same record. This is approach is somewhat different to other existing time series databases like kdb+/q, InfluxDB etc. where it's fairly straightforward to store multiple fields in the same record.\n",
    "\n",
    "\n",
    "### Further reading on Timestream\n",
    "\n",
    "Timestream is quite new so there isn't a huge amount of material available on the web for it, but it does appear to be slowly increasing. The first port of call is AWS's documentation which is very comprehensive at https://docs.aws.amazon.com/timestream/index.html.\n",
    "\n",
    "I'd also recommend reading this introduction to Timestream at https://dev.to/pblitz/aws-timestream-an-intro-4i1j\n",
    "\n",
    "AWS have put a sample Python app on GitHub which uses boto to access a Timestream database at https://github.com/awslabs/amazon-timestream-tools/tree/master/sample_apps/python for recording your CPU utilisation. We're going to use that as reference to play with Timestream from Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making AWS accessible via Python\n",
    "\n",
    "Given that Timestream is in the cloud, we need to make sure that AWS services need to be accessible from Python, whether we are running our process in the cloud (which seems preferable to reduce latency) or locally. Whilst we are using Python, Timestream is also accessible from many other langauges.\n",
    "\n",
    "* Hence, before going through this tutorial, you'll need to go through several steps so AWS services are accessible from your machine\n",
    "    * You'll need to create an IAM user, with appropriate permissions\n",
    "        * In our case this will to have permissions to use Timestream\n",
    "        * Get the Access key ID and secret access key for the IAM user\n",
    "    * Install AWS CLI\n",
    "        * run `sudo apt install awscli`\n",
    "        * or you can download the zip file\n",
    "        * run `aws configure` to set the default access key ID, default AWS availability zone etc.\n",
    "        * this will create files in ~/.aws/credentials and ~/.aws/config\n",
    "    * AWS CLI instructions at https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html#cliv2-linux-install\n",
    "\n",
    "* Once your  credentials are set, we can use boto3, which is an SDK for Python developers to access AWS resources:\n",
    "    * boto3 instructions https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "    * You can install boto3 using pip\n",
    "    * If you follow the instruction at https://github.com/cuemacro/teaching/blob/master/pythoncourse/installation/installing_anaconda_and_pycharm.ipynb - you'll create a conda environment `py37class` which includes boto3, and many useful data science libraries, which I use for my Python teaching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download tick data\n",
    "\n",
    "Let's first download some FX tick data from Dukascopy, which is a Swiss FX retail broker, which offers access to its dataset. We'll later store in Timestream. Create a folder under the working directory called `raw_data` for it doesn't already exists, where we'll store the data initally as flat files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:28.320559Z",
     "start_time": "2020-10-31T12:53:28.205457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘raw_data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download EURUSD tick data from Dukascopy for 2019 (note: this will take a while!) and dump to a Parquet file. If we re-run our code, it should pick up the Parquet file instead of downloading from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:31.057113Z",
     "start_time": "2020-10-31T12:53:28.322893Z"
    }
   },
   "outputs": [],
   "source": [
    "from findatapy.market import Market, MarketDataGenerator, MarketDataRequest\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "market = Market(market_data_generator=MarketDataGenerator())\n",
    "\n",
    "if os.path.exists('raw_data/EURUSD_2019.gzip'):\n",
    "    df_tick = pd.read_parquet('raw_data/EURUSD_2019.gzip')\n",
    "else:\n",
    "    md_request = MarketDataRequest(\n",
    "        start_date='01 Jan 2018', finish_date='01 Jan 2019',\n",
    "        fields=['bid', 'ask'], vendor_fields=['bid', 'ask'],\n",
    "        freq='tick', data_source='dukascopy',\n",
    "        tickers=['EURUSD'], vendor_tickers=['EURUSD'], category='fx')\n",
    "\n",
    "    df_tick = Market(market_data_generator=MarketDataGenerator()).fetch_market(md_request)\n",
    "    df_tick.to_parquet('raw_data/EURUSD_2019.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mid price column, which we'll use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:31.632044Z",
     "start_time": "2020-10-31T12:53:31.059458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-31 12:53:31,069 - numexpr.utils - INFO - Note: NumExpr detected 28 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2020-10-31 12:53:31,070 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "df_tick['mid'] = (df_tick['EURUSD.bid'] + df_tick['EURUSD.ask'])/2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print tick data to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:31.640686Z",
     "start_time": "2020-10-31T12:53:31.634181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         EURUSD.bid  EURUSD.ask       mid\n",
      "Date                                                     \n",
      "2019-01-01 22:02:37.254     1.14598     1.14682  1.146400\n",
      "2019-01-01 22:02:38.590     1.14599     1.14682  1.146405\n",
      "2019-01-01 22:02:39.138     1.14599     1.14684  1.146415\n",
      "2019-01-01 22:02:55.787     1.14598     1.14684  1.146410\n",
      "2019-01-01 22:03:02.060     1.14598     1.14684  1.146410\n",
      "2019-01-01 22:03:12.290     1.14599     1.14684  1.146415\n",
      "2019-01-01 22:03:16.253     1.14599     1.14684  1.146415\n",
      "2019-01-01 22:03:58.115     1.14607     1.14691  1.146490\n",
      "2019-01-01 22:03:59.146     1.14607     1.14678  1.146425\n",
      "2019-01-01 22:04:00.208     1.14607     1.14684  1.146455\n"
     ]
    }
   ],
   "source": [
    "print(df_tick.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Timestream database and table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to create a Timestream database and table, which we'll later populate with tick data.\n",
    "\n",
    "We need to create a session to interact with AWS, by using the boto3 Python library. This will pick up our various AWS keys which we've set earlier after installation of AWS CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:31.773127Z",
     "start_time": "2020-10-31T12:53:31.642657Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "session = boto3.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the clients for writing and reading to Timestream using boto3. Note, old versions of boto3, won't have the `timestream-write` or `timestream-query` services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:31.808858Z",
     "start_time": "2020-10-31T12:53:31.775333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-31 12:53:31,778 - botocore.utils - INFO - IMDS ENDPOINT: http://169.254.169.254/\n",
      "2020-10-31 12:53:31,784 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "# Recommended Timestream write client SDK configuration:\n",
    "#  - Set SDK retry count to 10.\n",
    "#  - Use SDK DEFAULT_BACKOFF_STRATEGY\n",
    "#  - Set RequestTimeout to 20 seconds .\n",
    "#  - Set max connections to 5000 or higher.\n",
    "write_client = session.client('timestream-write', config=Config(read_timeout=20, max_pool_connections=5000,\n",
    "                                                                    retries={'max_attempts': 10}))\n",
    "query_client = session.client('timestream-query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the database and table names. We also need to define the duration of the memory storage for Timestream (24h) and also the duration of the period for data to reside under magnetic tape (7 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:31.813353Z",
     "start_time": "2020-10-31T12:53:31.810559Z"
    }
   },
   "outputs": [],
   "source": [
    "DATABASE_NAME = \"fxtick\"\n",
    "TABLE_NAME = \"EURUSD\"\n",
    "HT_TTL_HOURS = 24\n",
    "CT_TTL_DAYS = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write functions for creating the database and the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:31.827589Z",
     "start_time": "2020-10-31T12:53:31.815624Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_database():\n",
    "        print(\"Creating Database\")\n",
    "        try:\n",
    "            write_client.create_database(DatabaseName=DATABASE_NAME)\n",
    "            print(\"Database [%s] created successfully.\" % DATABASE_NAME)\n",
    "        except write_client.exceptions.ConflictException:\n",
    "            print(\"Database [%s] exists. Skipping database creation\" % DATABASE_NAME)\n",
    "        except Exception as err:\n",
    "            print(\"Create database failed:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:31.840358Z",
     "start_time": "2020-10-31T12:53:31.829564Z"
    }
   },
   "outputs": [],
   "source": [
    " def create_table():\n",
    "        print(\"Creating table\")\n",
    "        retention_properties = {\n",
    "            'MemoryStoreRetentionPeriodInHours': HT_TTL_HOURS,\n",
    "            'MagneticStoreRetentionPeriodInDays': CT_TTL_DAYS\n",
    "        }\n",
    "        try:\n",
    "            write_client.create_table(DatabaseName=DATABASE_NAME, TableName=TABLE_NAME,\n",
    "                                     RetentionProperties=retention_properties)\n",
    "            print(\"Table [%s] successfully created.\" % TABLE_NAME)\n",
    "        except write_client.exceptions.ConflictException:\n",
    "            print(\"Table [%s] exists on database [%s]. Skipping table creation\" % (\n",
    "                TABLE_NAME, DATABASE_NAME))\n",
    "        except Exception as err:\n",
    "            print(\"Create table failed:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the database and the table. These steps will fail, if we already have them created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:32.264444Z",
     "start_time": "2020-10-31T12:53:31.842199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Database\n",
      "Database [fxtick] created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:32.581803Z",
     "start_time": "2020-10-31T12:53:32.266246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table\n",
      "Table [EURUSD] successfully created.\n"
     ]
    }
   ],
   "source": [
    "create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling the table with tick data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to get the current time (in Unix milliseconds), this will be used to populate the `Time` on each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:32.586529Z",
     "start_time": "2020-10-31T12:53:32.583604Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_current_time():\n",
    "    return str(int(round(time.time() * 1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes a DataFrame and converts the selected column into a `Measure`. This is then converted into a list of dictionaries. The final step is to take the timestamp, which we've got earlier and remap as the `venue_time`. The `Time` field is the time we snap here (ie. writing time). The for loop at the end is going to work quite slow. If you were using this in production, it would be worth spending time to optimize this code. There is an AWS project on GitHub, [AWS Data Wrangler](https://github.com/awslabs/aws-data-wrangler) that makes it easier to use Pandas on AWS, but I didn't have enough time to test that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:32.599574Z",
     "start_time": "2020-10-31T12:53:32.588033Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_df_to_dict(df, column):\n",
    "    \n",
    "    df_conv = pd.DataFrame(index=df.index, data=df[column].values, columns=['MeasureValue'])\n",
    "    df_conv.index.name = 'venue_time'\n",
    "    df_conv = df_conv.reset_index()\n",
    "    df_conv['venue_time'] = df_conv['venue_time'].astype(np.int64) // 10**6\n",
    "\n",
    "    df_conv['MeasureName'] = 'mid'\n",
    "    df_conv['MeasureValueType'] = 'DOUBLE'\n",
    "    \n",
    "    df_conv['MeasureValue'] = df_conv['MeasureValue'].apply(str)\n",
    "    df_conv['venue_time'] = df_conv['venue_time'].apply(str)\n",
    "    \n",
    "    records = df_conv.to_dict('records')\n",
    "            \n",
    "    for r in records:\n",
    "        r['Dimensions'] = [{'Name' : 'venue_time', 'Value' : r['venue_time']}]\n",
    "        r.pop('venue_time') # Remove this, because written as a Dimension\n",
    "        r['Time'] = get_current_time() \n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function dumps the data to Timestream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:32.612147Z",
     "start_time": "2020-10-31T12:53:32.601279Z"
    }
   },
   "outputs": [],
   "source": [
    " def write_dict(records):\n",
    "        print(\"Writing records\")\n",
    "\n",
    "        try:\n",
    "            result = write_client.write_records(DatabaseName=DATABASE_NAME, TableName=TABLE_NAME,\n",
    "                                              Records=records, CommonAttributes={})\n",
    "            print(\"WriteRecords Status: [%s]\" % result['ResponseMetadata']['HTTPStatusCode'])\n",
    "        except Exception as err:\n",
    "            print(\"Error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert a small number of points in our tick data, selecting the `mid` point, into a list of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:32.631855Z",
     "start_time": "2020-10-31T12:53:32.613685Z"
    }
   },
   "outputs": [],
   "source": [
    "tick_records = convert_df_to_dict(df_tick[0:50], 'mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now in a form that can be ingested by Timestream, ie. a list of dictionaries. Note, the nested `Dimensions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:32.644410Z",
     "start_time": "2020-10-31T12:53:32.633452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'MeasureValue': '1.146399974822998', 'MeasureName': 'mid', 'MeasureValueType': 'DOUBLE', 'Dimensions': [{'Name': 'venue_time', 'Value': '1546380157254'}], 'Time': '1604148812630'}, {'MeasureValue': '1.1464049816131592', 'MeasureName': 'mid', 'MeasureValueType': 'DOUBLE', 'Dimensions': [{'Name': 'venue_time', 'Value': '1546380158590'}], 'Time': '1604148812630'}, {'MeasureValue': '1.1464149951934814', 'MeasureName': 'mid', 'MeasureValueType': 'DOUBLE', 'Dimensions': [{'Name': 'venue_time', 'Value': '1546380159138'}], 'Time': '1604148812630'}, {'MeasureValue': '1.1464099884033203', 'MeasureName': 'mid', 'MeasureValueType': 'DOUBLE', 'Dimensions': [{'Name': 'venue_time', 'Value': '1546380175787'}], 'Time': '1604148812630'}, {'MeasureValue': '1.1464099884033203', 'MeasureName': 'mid', 'MeasureValueType': 'DOUBLE', 'Dimensions': [{'Name': 'venue_time', 'Value': '1546380182060'}], 'Time': '1604148812630'}]\n"
     ]
    }
   ],
   "source": [
    "print(tick_records[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write our list of records to our Timestream database. Note, that sometimes we might get some error associated with the upload, such as rate limit exceeded in Jupyter notebook. If this is the case, we simply start our Jupyter notebook with this additional command line parameter `--NotebookApp.iopub_data_rate_limit=1000000000`.\n",
    "\n",
    "This might take a while to run, particularly, if your not running it on an EC2 instance in the same availability zone. We'd also suggest parallelizing this write function, to cut down the number of records your write. There is a limit to the amount of data you push to Timestream, which can cause an exception if breached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:32.812998Z",
     "start_time": "2020-10-31T12:53:32.646224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing records\n",
      "WriteRecords Status: [200]\n"
     ]
    }
   ],
   "source": [
    "write_dict(tick_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the tick data in the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already written the tick data to Timestream. Let's see if we can fetch it. We now create a Query object which runs any query we write. It will also iterate through the output to print it to screen. This code is taken from GitHub https://github.com/awslabs/amazon-timestream-tools/blob/master/sample_apps/python/QueryExample.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:32.826853Z",
     "start_time": "2020-10-31T12:53:32.814691Z"
    }
   },
   "outputs": [],
   "source": [
    "class Query(object):\n",
    "\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.paginator = client.get_paginator('query')\n",
    "\n",
    "    # See records ingested into this table so far\n",
    "    SELECT_ALL = f\"SELECT * FROM {DATABASE_NAME}.{TABLE_NAME}\"\n",
    "\n",
    "    def run_query(self, query_string):\n",
    "        try:\n",
    "            page_iterator = self.paginator.paginate(QueryString=query_string)\n",
    "            for page in page_iterator:\n",
    "                self._parse_query_result(page)\n",
    "        except Exception as err:\n",
    "            print(\"Exception while running query:\", err)\n",
    "\n",
    "    def _parse_query_result(self, query_result):\n",
    "        column_info = query_result['ColumnInfo']\n",
    "\n",
    "        print(\"Metadata: %s\" % column_info)\n",
    "        print(\"Data: \")\n",
    "        for row in query_result['Rows']:\n",
    "            print(self._parse_row(column_info, row))\n",
    "\n",
    "    def _parse_row(self, column_info, row):\n",
    "        data = row['Data']\n",
    "        row_output = []\n",
    "        for j in range(len(data)):\n",
    "            info = column_info[j]\n",
    "            datum = data[j]\n",
    "            row_output.append(self._parse_datum(info, datum))\n",
    "\n",
    "        return \"{%s}\" % str(row_output)\n",
    "\n",
    "    def _parse_datum(self, info, datum):\n",
    "        if datum.get('NullValue', False):\n",
    "            return \"%s=NULL\" % info['Name'],\n",
    "\n",
    "        column_type = info['Type']\n",
    "\n",
    "        # If the column is of TimeSeries Type\n",
    "        if 'TimeSeriesMeasureValueColumnInfo' in column_type:\n",
    "            return self._parse_time_series(info, datum)\n",
    "\n",
    "        # If the column is of Array Type\n",
    "        elif 'ArrayColumnInfo' in column_type:\n",
    "            array_values = datum['ArrayValue']\n",
    "            return \"%s=%s\" % (info['Name'], self._parse_array(info['Type']['ArrayColumnInfo'], array_values))\n",
    "\n",
    "        # If the column is of Row Type\n",
    "        elif 'RowColumnInfo' in column_type:\n",
    "            row_column_info = info['Type']['RowColumnInfo']\n",
    "            row_values = datum['RowValue']\n",
    "            return self._parse_row(row_column_info, row_values)\n",
    "\n",
    "        # If the column is of Scalar Type\n",
    "        else:\n",
    "            return self._parse_column_name(info) + datum['ScalarValue']\n",
    "\n",
    "    def _parse_time_series(self, info, datum):\n",
    "        time_series_output = []\n",
    "        for data_point in datum['TimeSeriesValue']:\n",
    "            time_series_output.append(\"{time=%s, value=%s}\"\n",
    "                                      % (data_point['Time'],\n",
    "                                         self._parse_datum(info['Type']['TimeSeriesMeasureValueColumnInfo'],\n",
    "                                                           data_point['Value'])))\n",
    "        return \"[%s]\" % str(time_series_output)\n",
    "\n",
    "    def _parse_array(self, array_column_info, array_values):\n",
    "        array_output = []\n",
    "        for datum in array_values:\n",
    "            array_output.append(self._parse_datum(array_column_info, datum))\n",
    "\n",
    "        return \"[%s]\" % str(array_output)\n",
    "\n",
    "    def run_query_with_multiple_pages(self, limit):\n",
    "        query_with_limit = self.SELECT_ALL + \" LIMIT \" + str(limit)\n",
    "        print(\"Starting query with multiple pages : \" + query_with_limit)\n",
    "        self.run_query(query_with_limit)\n",
    "\n",
    "    def cancel_query(self):\n",
    "        print(\"Starting query: \" + self.SELECT_ALL)\n",
    "        result = self.client.query(QueryString=self.SELECT_ALL)\n",
    "        print(\"Cancelling query: \" + self.SELECT_ALL)\n",
    "        try:\n",
    "            self.client.cancel_query(QueryId=result['QueryId'])\n",
    "            print(\"Query has been successfully cancelled\")\n",
    "        except Exception as err:\n",
    "            print(\"Cancelling query failed:\", err)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_column_name(info):\n",
    "        if 'Name' in info:\n",
    "            return info['Name'] + \"=\"\n",
    "        else:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the Query object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:32.849416Z",
     "start_time": "2020-10-31T12:53:32.828514Z"
    }
   },
   "outputs": [],
   "source": [
    "query = Query(query_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the query, which is simply to output the time of writing, `venue_time` and the `mid` price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:32.854932Z",
     "start_time": "2020-10-31T12:53:32.851488Z"
    }
   },
   "outputs": [],
   "source": [
    "QUERY_1 = f\"\"\"\n",
    "        SELECT time, venue_time, measure_value::double\n",
    "        FROM {DATABASE_NAME}.{TABLE_NAME} ORDER BY venue_time, time DESC LIMIT 1000 \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the the query and print the output, to show the data to the user. In practice, a next obvious step would be rewrite the Query object, so that it parse the output and convert into a pandas DataFrame, rather than simply printing every row to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:33.451817Z",
     "start_time": "2020-10-31T12:53:32.857428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: [{'Name': 'time', 'Type': {'ScalarType': 'TIMESTAMP'}}, {'Name': 'venue_time', 'Type': {'ScalarType': 'VARCHAR'}}, {'Name': 'measure_value::double', 'Type': {'ScalarType': 'DOUBLE'}}]\n",
      "Data: \n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380157254', 'measure_value::double=1.146399974822998']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380158590', 'measure_value::double=1.1464049816131592']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380159138', 'measure_value::double=1.1464149951934814']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380175787', 'measure_value::double=1.1464099884033203']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380182060', 'measure_value::double=1.1464099884033203']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380192290', 'measure_value::double=1.1464149951934814']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380196253', 'measure_value::double=1.1464149951934814']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380238115', 'measure_value::double=1.1464899778366089']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380239146', 'measure_value::double=1.1464250087738037']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380240208', 'measure_value::double=1.1464550495147705']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380246655', 'measure_value::double=1.1463799476623535']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380268222', 'measure_value::double=1.1464550495147705']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380283687', 'measure_value::double=1.146359920501709']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380285411', 'measure_value::double=1.146359920501709']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380299153', 'measure_value::double=1.1463549137115479']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380319167', 'measure_value::double=1.1463549137115479']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380338220', 'measure_value::double=1.146440029144287']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380339029', 'measure_value::double=1.1463749408721924']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380356415', 'measure_value::double=1.1463749408721924']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380357647', 'measure_value::double=1.1463749408721924']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380358220', 'measure_value::double=1.146435022354126']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380358271', 'measure_value::double=1.1464600563049316']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380358496', 'measure_value::double=1.1465349197387695']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380359207', 'measure_value::double=1.1465399265289307']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380369897', 'measure_value::double=1.146530032157898']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380371010', 'measure_value::double=1.146530032157898']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380371113', 'measure_value::double=1.146530032157898']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380371215', 'measure_value::double=1.146530032157898']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380371322', 'measure_value::double=1.146530032157898']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380371471', 'measure_value::double=1.146530032157898']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380371574', 'measure_value::double=1.146530032157898']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380371675', 'measure_value::double=1.146530032157898']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380372586', 'measure_value::double=1.1465349197387695']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380380463', 'measure_value::double=1.1464550495147705']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380380514', 'measure_value::double=1.146554946899414']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380381700', 'measure_value::double=1.1465049982070923']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380383115', 'measure_value::double=1.1463499069213867']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380386520', 'measure_value::double=1.146554946899414']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380397009', 'measure_value::double=1.146435022354126']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380397111', 'measure_value::double=1.1465849876403809']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380397162', 'measure_value::double=1.1466100215911865']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380397213', 'measure_value::double=1.1464600563049316']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380397264', 'measure_value::double=1.146440029144287']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380397315', 'measure_value::double=1.1464300155639648']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380397504', 'measure_value::double=1.1464200019836426']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380397606', 'measure_value::double=1.1464200019836426']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380397823', 'measure_value::double=1.1463350057601929']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380397925', 'measure_value::double=1.1463100910186768']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380398975', 'measure_value::double=1.1463100910186768']}\n",
      "{['time=2020-10-31 12:53:32.630000000', 'venue_time=1546380399077', 'measure_value::double=1.1462650299072266']}\n"
     ]
    }
   ],
   "source": [
    "query_output = query.run_query(QUERY_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up database\n",
    "\n",
    "Before we leave, let's define functions to delete the table and the database we've just written, so we don't incur any unnecessary storage costs on Timestream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:33.461131Z",
     "start_time": "2020-10-31T12:53:33.457222Z"
    }
   },
   "outputs": [],
   "source": [
    "def delete_table():\n",
    "        print(\"Deleting Table\")\n",
    "        try:\n",
    "            result = write_client.delete_table(DatabaseName=DATABASE_NAME, TableName=TABLE_NAME)\n",
    "            print(\"Delete table status [%s]\" % result['ResponseMetadata']['HTTPStatusCode'])\n",
    "        except write_client.exceptions.ResourceNotFoundException:\n",
    "            print(\"Table [%s] doesn't exist\" % TABLE_NAME)\n",
    "        except Exception as err:\n",
    "            print(\"Delete table failed:\", err)\n",
    "\n",
    "def delete_database():\n",
    "        print(\"Deleting Database\")\n",
    "        try:\n",
    "            result = write_client.delete_database(DatabaseName=DATABASE_NAME)\n",
    "            print(\"Delete database status [%s]\" % result['ResponseMetadata']['HTTPStatusCode'])\n",
    "        except self.client.exceptions.ResourceNotFoundException:\n",
    "            print(\"database [%s] doesn't exist\" % DATABASE_NAME)\n",
    "        except Exception as err:\n",
    "            print(\"Delete database failed:\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run both functions to delete the table and then the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:33.806367Z",
     "start_time": "2020-10-31T12:53:33.463657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Table\n",
      "Delete table status [200]\n"
     ]
    }
   ],
   "source": [
    "delete_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T12:53:33.880007Z",
     "start_time": "2020-10-31T12:53:33.808248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Database\n",
      "Delete database status [200]\n"
     ]
    }
   ],
   "source": [
    "delete_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've seen how it's fairly easy to use Timestream to store tick data. I have only stored a relatively small number of points, so I've not really tested out how it scales (which is supposed to be the big benefit of using it!). As noted, to make this into production code, we'd likely to need to create highly optimized code for conversion of Pandas DataFrames, back and forth into formats useful for Timestream. It would also likely be necessary to parallelize the code both for writing to Timestream and also for querying the dataset (indeed, this is what I've done in my tcapy library, when fetching tick data).\n",
    "\n",
    "If you have any feedback about Timestream, I'd be very interested to hear it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
